{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c722e2-76d3-4e94-8c45-2f789805a551",
   "metadata": {},
   "source": [
    "This script has been modified for 5 seconds video duration on April 29th  2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chief-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from moviepy.editor import VideoFileClip\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import tqdm\n",
    "import heapq\n",
    "import datetime\n",
    "import glob\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from absl import logging\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "# Some modules to help with reading the UCF101 dataset.\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "import ssl\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython import display\n",
    "from youtube_dl import YoutubeDL\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a137ca6-eb61-4e35-b90a-8615bc15a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from moviepy.editor import VideoFileClip\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import tqdm\n",
    "import heapq\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "import ssl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mental-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=os.getcwd()\n",
    "BASE_PATH = PATH+'/newdataset/'\n",
    "VIDEOS_PATH = os.path.join(BASE_PATH, '**','*.mp4')\n",
    "FILE_SELECTOR = \"**/*.mp4\"\n",
    "SEQUENCE_LENGTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gross-composite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defensive-greene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0', '/device:GPU:1')\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.AUTO\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-headquarters",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "soviet-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to see the videos that are loaded in Gif format. pass in the output of generator. The np.array only\n",
    "\n",
    "def to_gif(images):\n",
    "    #Code to see the videos that are loaded in Gif format. pass in the output of generator. The np.array only\n",
    "    converted_images =np.clip(images.numpy()[0]* 255, 0, 255).astype(np.uint8)\n",
    "    gifnum=len([f for f in glob.glob('/home/adewopva/Codes/'+\"**/*.gif\", recursive=True)])+1\n",
    "    imageio.mimsave('./animation'+str(gifnum)+'.gif', converted_images, fps=25)\n",
    "    #IPython.display.Image('animation.gif')\n",
    "    return embed.embed_file('./animation'+str(gifnum)+'.gif')\n",
    "    ![SegmentLocal](191px-Seven_segment_display-animation.gif \"segment\")\n",
    "    \n",
    "\n",
    "# to_gif(***sample_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "introductory-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for predicting Test video\n",
    "def video_test(video_paths):\n",
    "\n",
    "    def crop_center_square(frame):\n",
    "        (y, x) = frame.shape[0:2]\n",
    "        min_dim = min(y, x)\n",
    "        start_x = x // 2 - min_dim // 2\n",
    "        start_y = y // 2 - min_dim // 2\n",
    "        return frame[start_y:start_y + min_dim, start_x:start_x\n",
    "                     + min_dim]\n",
    "\n",
    "    max_frames = 150\n",
    "    cap = cv2.VideoCapture(video_paths)\n",
    "    #frames = []\n",
    "    frames = np.zeros(shape=(max_frames, 224, 224, 3))\n",
    "    i =0\n",
    "    try:\n",
    "        while True:\n",
    "            (ret, frame) = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, (224, 224))\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "\n",
    "            frames[i]=frame\n",
    "            i+=1\n",
    "            if i==150:\n",
    "                break\n",
    "#                 frames.append(frame)\n",
    "\n",
    "#                 if len(frames) == max_frames:\n",
    "#                     break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    #yield np.array(frames) / 255.0, Label\n",
    "    return (tf.constant(frames, dtype=tf.float32)[tf.newaxis, ...])/ 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-unknown",
   "metadata": {},
   "source": [
    "#Load all the threee saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "short-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all Saved model\n",
    "K_I_512D= tf.keras.models.load_model(PATH+'/saved_model/NoDrop_NoDense_5sec_AR_kinetics+ImageNetweightsonly')\n",
    "K_I_aug_20E= tf.keras.models.load_model(PATH+'/saved_model/augumented_5sec_AR_kinetics+ImageNetweightsonly')\n",
    "K_3DL= tf.keras.models.load_model(PATH+'/saved_model/5sec_AR_kineticsweightsonly_noflatten_moredense')\n",
    "# Check its architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3de6c46d-e8c2-46c3-8c76-9c67ae53e951",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-programmer",
   "metadata": {},
   "source": [
    "#Traditional prediction with output of result\n",
    "result=new_model.predict(clip)\n",
    "print(result)\n",
    "result=result.argmax(1)\n",
    "result"
   ]
  },
  {
   "cell_type": "raw",
   "id": "supposed-active",
   "metadata": {},
   "source": [
    "#Show gif of the video\n",
    "to_gif(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All embedded script to split, get duration, crop., preprocess videos and make predictions\n",
    "import math\n",
    "from IPython.display import Video\n",
    "def all_embed(videopath):\n",
    "    labels=['on_feet', 'active', 'rest', 'escape', 'crawling']\n",
    "    \n",
    "    def get_duration(path):      #Get duration of clips function\n",
    "        clip= cv2.VideoCapture(path)\n",
    "        totalframes = int(clip.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = int(clip.get(cv2.CAP_PROP_FPS))\n",
    "        rawduration= int(totalframes / fps)\n",
    "        #clip = VideoFileClip(path)\n",
    "        #rawduration= clip.duration\n",
    "        return rawduration\n",
    "    \n",
    "    def get_dir(filename): #Get the path name for any input\n",
    "        Fname=filename.split('/')[-1]\n",
    "        file=Fname.split('.')\n",
    "        dir=file[0].split('/')[-1]\n",
    "        return dir\n",
    "    def to_gif(images):\n",
    "        #Code to see the videos that are loaded in Gif format. pass in the output of generator. The np.array only\n",
    "        converted_images =np.clip(images.numpy()[0] * 255, 0, 255).astype(np.uint8)\n",
    "        gifnum=len([f for f in glob.glob('/home/adewopva/Codes/'+\"**/*.gif\", recursive=True)])+1\n",
    "        imageio.mimsave('./animation'+str(gifnum)+'.gif', converted_images, fps=25)\n",
    "        #IPython.display.Image('animation.gif')\n",
    "        return embed.embed_file('./animation'+str(gifnum)+'.gif')\n",
    "        ![SegmentLocal](191px-Seven_segment_display-animation.gif \"segment\")\n",
    "    def video_test(video_paths):\n",
    "\n",
    "        def crop_center_square(frame):\n",
    "            (y, x) = frame.shape[0:2]\n",
    "            min_dim = min(y, x)\n",
    "            start_x = x // 2 - min_dim // 2\n",
    "            start_y = y // 2 - min_dim // 2\n",
    "            return frame[start_y:start_y + min_dim, start_x:start_x\n",
    "                         + min_dim]\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_paths)\n",
    "        max_frames =150 #int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        #frames = []\n",
    "        frames = np.zeros(shape=(max_frames, 224, 224, 3))\n",
    "        i =0\n",
    "        try:\n",
    "            while True:\n",
    "                (ret, frame) = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = crop_center_square(frame)\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                frame = frame[:, :, [2, 1, 0]]\n",
    "\n",
    "                frames[i]=frame\n",
    "                i+=1\n",
    "                if i==150: #max_frames:\n",
    "                    break\n",
    "        #                 frames.append(frame)\n",
    "\n",
    "        #                 if len(frames) == max_frames:\n",
    "        #                     break\n",
    "        finally:\n",
    "            cap.release()\n",
    "        #yield np.array(frames) / 255.0, Label\n",
    "        return (tf.constant(frames, dtype=tf.float32)[tf.newaxis, ...])/ 255.0\n",
    "\n",
    "\n",
    "    def split_videos(videopath):#Split videos\n",
    "        rawduration=math.ceil(get_duration(videopath))\n",
    "        divisorrate=math.ceil(rawduration/5)    #Get the divsion rate to know how many number of iterations will be done on the video\n",
    "        newtime=0\n",
    "        nextstart=5\n",
    "        print('This Video will be splitted to :', divisorrate,'subvideos')\n",
    "        #print(rawduration)\n",
    "        #print(divisorrate)\n",
    "        for i in range(divisorrate):\n",
    "            dir=get_dir(videopath)\n",
    "            !mkdir -p \"$dir\"\n",
    "            if rawduration-nextstart>5:\n",
    "\n",
    "                filename = dir+ \"/\"+'splitted'+ '-'+str(newtime)+ \"-\" +str(nextstart)+ \".mp4\"\n",
    "                !ffmpeg -hide_banner -loglevel warning -i \"$videopath\" -ss $newtime -t 5 -c:v libx264 -crf 30  \"$filename\"\n",
    "                #nextstart=newtime+15\n",
    "            elif rawduration-nextstart<0:\n",
    "                newtime=(rawduration-5)\n",
    "\n",
    "                filename = dir+ \"/\"+'splitted'+ '-'+str(newtime)+ \"-\" +str(rawduration)+ \".mp4\"\n",
    "                !ffmpeg -hide_banner -loglevel warning -i \"$videopath\" -ss $newtime -t 5 -c:v libx264 -crf 30  \"$filename\"\n",
    "            else:\n",
    "                newtime=(rawduration-5)-(rawduration-nextstart)\n",
    "\n",
    "                filename = dir+ \"/\"+'splitted'+ '-'+str(newtime)+ \"-\" +str(nextstart)+ \".mp4\"\n",
    "                !ffmpeg -hide_banner -loglevel warning -i \"$videopath\" -ss $newtime -t 5 -c:v libx264 -crf 30  \"$filename\"\n",
    "            newtime=nextstart\n",
    "            nextstart=newtime+5\n",
    "       \n",
    "    def iter_predict(videopath):\n",
    "        if get_duration(videopath)>6:\n",
    "            BASE_PATH = get_dir(videopath)\n",
    "            FILE_SELECTOR = \"**/*.mp4\"\n",
    "            files = [f for f in glob.glob(BASE_PATH + '/' + FILE_SELECTOR, recursive=True)]\n",
    "            files = sorted(files, key=lambda x:float(re.findall(\"(\\d+)\",x)[0]))\n",
    "            for file in files:\n",
    "                prediction_range= (file.split('/splitted-')[-1]).split('.')[0]\n",
    "                print('\\n')\n",
    "                print('Actions Detected for %s seconds of the input video are:'%(prediction_range))\n",
    "                clip=video_test(file)\n",
    "               # print(clip.shape)\n",
    "                K_I_512D_result= K_I_512D.predict(clip)[0]\n",
    "                probabilities1 = tf.nn.softmax(K_I_512D_result)\n",
    "                #print(kin_RGB_k_Iresultresult)\n",
    "                print('------------------------------------------------------------------') \n",
    "                print(\"The Kinetics and Imagenet 512D model predictions are:\")\n",
    "                for i in np.argsort(probabilities1)[::-1][:3]:\n",
    "                    print(f\"  {labels[i]:}: {probabilities1[i] * 100:5.2f}%\")\n",
    "                    #print('\\n')\n",
    "                print('------------------------------------------------------------------') \n",
    "                K_3DL_result= K_3DL.predict(clip)[0]\n",
    "                probabilities2 = tf.nn.softmax(K_3DL_result)\n",
    "                #print(kin_RGB_kresultresult)\n",
    "                print(\"The Kinetics 3DL model predictions are:\")\n",
    "                for i in np.argsort(probabilities2)[::-1][:3]:\n",
    "                    print(f\"  {labels[i]:}: {probabilities2[i] * 100:5.2f}%\")\n",
    "                print('------------------------------------------------------------------') \n",
    "                K_I_aug_20E_result= K_I_aug_20E.predict(clip)[0]\n",
    "                probabilities3 = tf.nn.softmax(K_I_aug_20E_result)\n",
    "                #print(kin_RGB_kresultresult)\n",
    "                print(\"The Kinetics and Imagenet video_augmented model predictions are:\")\n",
    "                for i in np.argsort(probabilities3)[::-1][:3]:\n",
    "                    print(f\"  {labels[i]:}: {probabilities3[i] * 100:5.2f}%\")\n",
    "                               \n",
    "        else:\n",
    "            clip=video_test(videopath)\n",
    "            #print(clip.shape)\n",
    "            K_I_512D_result= K_I_512D.predict(clip)[0]\n",
    "            probabilities1 = tf.nn.softmax(K_I_512D_result)\n",
    "            #print(kin_RGB_k_Iresultresult)\n",
    "            print('------------------------------------------------------------------') \n",
    "            print(\"The Kinetics and Imagenet 512D model predictions are:\")\n",
    "            for i in np.argsort(probabilities1)[::-1][:3]:\n",
    "                print(f\"  {labels[i]:}: {probabilities1[i] * 100:5.2f}%\")\n",
    "            print('------------------------------------------------------------------')  \n",
    "            K_3DL_result= K_3DL.predict(clip)[0]\n",
    "            probabilities2 = tf.nn.softmax(K_3DL_result)\n",
    "            #print(kin_RGB_kresultresult)\n",
    "            print(\"The Kinetics 3DL model predictions are:\")\n",
    "            for i in np.argsort(probabilities2)[::-1][:3]:\n",
    "                print(f\"  {labels[i]:}: {probabilities2[i] * 100:5.2f}%\")\n",
    "            print('------------------------------------------------------------------') \n",
    "            K_I_aug_20E_result= K_I_aug_20E.predict(clip)[0]\n",
    "            probabilities3 = tf.nn.softmax(K_I_aug_20E_result)\n",
    "            #print(kin_RGB_kresultresult)\n",
    "            print(\"The Kinetics and Imagenet video_augmented model predictions are:\")\n",
    "            for i in np.argsort(probabilities3)[::-1][:3]:\n",
    "                print(f\"  {labels[i]:}: {probabilities3[i] * 100:5.2f}%\")\n",
    "\n",
    "\n",
    "    if get_duration(videopath)>6: \n",
    "        split_videos(videopath)\n",
    "        iter_predict(videopath)\n",
    "    else:\n",
    "        iter_predict(videopath)\n",
    "        Video(videopath, embed=True, height=240,width=240)\n",
    "    return to_gif(video_test(videopath))\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b4476-7978-4cf5-85ac-ce3fdc7a17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gif(video_test('/home/adewopva/Codes/multiple_activities/splitted-85-90.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe9e58-37e6-43d0-b455-83d6f95155db",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embed('/home/adewopva/Downloads/TestVideosForPhase1(Action Recognition and Prediction)/multiple_activities.mov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cff98a-f5e5-4c43-900c-8d52bab4989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gif(video_test('/home/adewopva/Codes/multiple_activities/splitted-15-20.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6546fc-919e-4f4c-a3d1-91f67b6bc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!youtube-dl 'https://www.pexels.com/video/baby-girl-crawling-on-the-floor-3196565/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08673c6-7969-40b9-a115-f01b8437fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embed(\"/home/adewopva/Codes/pexels-sarah-chai-7281356.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e6b30-a2ba-443e-8f4c-342d105908b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gif(video_test('/home/adewopva/Codes/pexels-sarah-chai-7281356/splitted-0-5.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e2aad-233d-4536-af8a-0c9e106fa1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embed(\"/home/adewopva/Codes/video-371861428.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len([f for f in glob.glob('/home/adewopva/OneDrive/Independent_Study/Dr.Nel'+\n",
    "#                           'ly/ROUGH Scripts/AR_UCF/Testing_Models/'+\"**/*.gif\", recursive=True)])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recognize Actions using the Kinetics+Imagenet weights Model or Kinetics Model only.\n",
    "#To use this script the video path must be specified with also the weights/model to use\n",
    "import sys\n",
    "import math\n",
    "from IPython.display import Video\n",
    "def Allscripts_Recognition(videopath,modelweight):\n",
    "    labels=['on_feet', 'active', 'rest', 'escape', 'crawling']\n",
    "    weights=modelweight\n",
    "    \n",
    "    def get_duration(path):      #Get duration of clips function\n",
    "        clip= cv2.VideoCapture(path)\n",
    "        totalframes = int(clip.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = int(clip.get(cv2.CAP_PROP_FPS))\n",
    "        rawduration= int(totalframes / fps)\n",
    "        #clip = VideoFileClip(path)\n",
    "        #rawduration= clip.duration\n",
    "        return rawduration\n",
    "    \n",
    "    def get_dir(filename): #Get the path name for any input\n",
    "        Fname=filename.split('/')[-1]\n",
    "        file=Fname.split('.')\n",
    "        dir=file[0].split('/')[-1]\n",
    "        return dir\n",
    "    def to_gif(images):\n",
    "        #Code to see the videos that are loaded in Gif format. pass in the output of generator. The np.array only\n",
    "        converted_images =np.clip(images.numpy()[0] * 255, 0, 255).astype(np.uint8)\n",
    "        gifnum=len([f for f in glob.glob('/home/adewopva/OneDrive/Independent_Study/Dr.Nelly'+\n",
    "                '/ROUGH Scripts/AR_UCF/Testing_Models/'+\"**/*.gif\", recursive=True)])+1\n",
    "        imageio.mimsave('./animation'+str(gifnum)+'.gif', converted_images, fps=25)\n",
    "        #IPython.display.Image('animation.gif')\n",
    "        return embed.embed_file('./animation'+str(gifnum)+'.gif')\n",
    "        ![SegmentLocal](191px-Seven_segment_display-animation.gif \"segment\")\n",
    "\n",
    "    def video_test(video_paths):\n",
    "\n",
    "        def crop_center_square(frame):\n",
    "            (y, x) = frame.shape[0:2]\n",
    "            min_dim = min(y, x)\n",
    "            start_x = x // 2 - min_dim // 2\n",
    "            start_y = y // 2 - min_dim // 2\n",
    "            return frame[start_y:start_y + min_dim, start_x:start_x\n",
    "                         + min_dim]\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_paths)\n",
    "        max_frames =150 #int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        #frames = []\n",
    "        frames = np.zeros(shape=(max_frames, 224, 224, 3))\n",
    "        i =0\n",
    "        try:\n",
    "            while True:\n",
    "                (ret, frame) = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = crop_center_square(frame)\n",
    "                frame = cv2.resize(frame, (224, 224))\n",
    "                frame = frame[:, :, [2, 1, 0]]\n",
    "\n",
    "                frames[i]=frame\n",
    "                i+=1\n",
    "                if i==150: #max_frames:\n",
    "                    break\n",
    "        #                 frames.append(frame)\n",
    "\n",
    "        #                 if len(frames) == max_frames:\n",
    "        #                     break\n",
    "        finally:\n",
    "            cap.release()\n",
    "        #yield np.array(frames) / 255.0, Label\n",
    "        return (tf.constant(frames, dtype=tf.float32)[tf.newaxis, ...])/ 255.0\n",
    "\n",
    "\n",
    "\n",
    "    def split_videos(videopath):#Split videos\n",
    "        rawduration=math.ceil(get_duration(videopath))\n",
    "        divisorrate=math.ceil(rawduration/5)    #Get the divsion rate to know how many number of iterations will be done on the video\n",
    "        newtime=0\n",
    "        nextstart=5\n",
    "        print('This Video will be splitted to :', divisorrate,'subvideos')\n",
    "        #print(rawduration)\n",
    "        #print(divisorrate)\n",
    "        for i in range(divisorrate):\n",
    "            dir=get_dir(videopath)\n",
    "            !mkdir -p \"$dir\"\n",
    "            if rawduration-nextstart>5:\n",
    "\n",
    "                filename = dir+ \"/\"+'splitted'+ '-'+str(newtime)+ \"-\" +str(nextstart)+ \".mp4\"\n",
    "                !ffmpeg -hide_banner -loglevel warning -i \"$videopath\" -ss $newtime -t 5 -c:v libx264 -crf 30  \"$filename\"\n",
    "                #nextstart=newtime+15\n",
    "            elif rawduration-nextstart<0:\n",
    "                newtime=(rawduration-5)\n",
    "\n",
    "                filename = dir+ \"/\"+'splitted'+ '-'+str(newtime)+ \"-\" +str(rawduration)+ \".mp4\"\n",
    "                !ffmpeg -hide_banner -loglevel warning -i \"$videopath\" -ss $newtime -t 5 -c:v libx264 -crf 30  \"$filename\"\n",
    "            else:\n",
    "                newtime=(rawduration-5)-(rawduration-nextstart)\n",
    "\n",
    "                filename = dir+ \"/\"+'splitted'+ '-'+str(newtime)+ \"-\" +str(nextstart)+ \".mp4\"\n",
    "                !ffmpeg -hide_banner -loglevel warning -i \"$videopath\" -ss $newtime -t 5 -c:v libx264 -crf 30  \"$filename\"\n",
    "            newtime=nextstart\n",
    "            nextstart=newtime+5\n",
    "       \n",
    "       \n",
    "    def iter_predict(videopath):\n",
    "        if get_duration(videopath)>6:\n",
    "            BASE_PATH = get_dir(videopath)\n",
    "            FILE_SELECTOR = \"**/*.mp4\"\n",
    "            files = [f for f in glob.glob(BASE_PATH + '/' + FILE_SELECTOR, recursive=True)]\n",
    "            files = sorted(files, key=lambda x:float(re.findall(\"(\\d+)\",x)[0]))\n",
    "            for file in files:\n",
    "                prediction_range= (file.split('/splitted-')[-1]).split('.')[0]\n",
    "                print('\\n')\n",
    "                print('For %s seconds of the input video:'%(prediction_range))\n",
    "                clip=video_test(file)\n",
    "               # print(clip.shape)\n",
    "                if weights=='Kinetics+ImageNet':\n",
    "                    kin_RGB_k_Iresult= kin_RGB_k_I.predict(clip)[0]\n",
    "                    probabilities1 = tf.nn.softmax(kin_RGB_k_Iresult)\n",
    "                    #print(kin_RGB_k_Iresultresult)\n",
    "                    print('The recognized baby action is : %s' %(labels[kin_RGB_k_Iresult.argmax()].upper()))\n",
    "                    print('------------------------------------------------------------------') \n",
    "                    print(\"Probability distribution for the 5 classes are:\")\n",
    "                    for i in np.argsort(probabilities1)[::-1][:5]:\n",
    "                        print(f\"  {labels[i]:}: {probabilities1[i] * 100:5.2f}%\")\n",
    "                        #print('\\n') \n",
    "                elif weights=='Kinetics':\n",
    "                    kin_RGB_kresult= kin_RGB_k.predict(clip)[0]\n",
    "                    probabilities2 = tf.nn.softmax(kin_RGB_kresult)\n",
    "                    print('The recognized baby action is : %s' %(labels[kin_RGB_kresult.argmax()].upper()))\n",
    "                    #print(probabilities1)\n",
    "                    print('------------------------------------------------------------------')\n",
    "                    print(\"Probability distribution for the 5 classes are:\")\n",
    "                    for i in np.argsort(probabilities2)[::-1][:5]:\n",
    "                        print(f\"  {labels[i]:}: {probabilities2[i] * 100:5.2f}%\")\n",
    "                else:\n",
    "                    print('The type of model weights used is incorrect')\n",
    "                    sys.exit()\n",
    "        else:\n",
    "            clip=video_test(videopath)\n",
    "            #print(clip.shape)\n",
    "            if weights=='Kinetics+ImageNet':\n",
    "                kin_RGB_k_Iresult= kin_RGB_k_I.predict(clip)[0]\n",
    "                probabilities1 = tf.nn.softmax(kin_RGB_k_Iresult)\n",
    "                print('The recognized baby action is : %s' %(labels[kin_RGB_k_Iresult.argmax()].upper()))                #print(probabilities1)\n",
    "                print('------------------------------------------------------------------')\n",
    "                print(\"Probability distribution for the 5 classes are:\")\n",
    "                for i in np.argsort(probabilities1)[::-1][:5]:\n",
    "                    print(f\"  {labels[i]:}: {probabilities1[i] * 100:5.2f}%\")\n",
    "                print('------------------------------------------------------------------')\n",
    "            elif weights=='Kinetics':\n",
    "                kin_RGB_kresult= kin_RGB_k.predict(clip)[0]\n",
    "                probabilities2 = tf.nn.softmax(kin_RGB_kresult)\n",
    "                print('The recognized baby action is : %s' %(labels[kin_RGB_kresult.argmax()].upper()))\n",
    "                #print(kin_RGB_kresultresult)\n",
    "                print('------------------------------------------------------------------')\n",
    "                print(\"Probability distribution for the 5 classes are:\")\n",
    "                for i in np.argsort(probabilities2)[::-1][:5]:\n",
    "                    print(f\"  {labels[i]:}: {probabilities2[i] * 100:5.2f}%\")\n",
    "                print('------------------------------------------------------------------')\n",
    "            else:\n",
    "                print('The type of model weights used is incorrect')\n",
    "                sys.exit()\n",
    "            \n",
    "\n",
    "    if get_duration(videopath)>6:\n",
    "        split_videos(videopath)\n",
    "        iter_predict(videopath)\n",
    "    else:\n",
    "        iter_predict(videopath)\n",
    "        Video(videopath, embed=True, height=240,width=240)\n",
    "    return to_gif(video_test(videopath))\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "!youtube-dl 'https://www.instagram.com/p/CNtG03dAJSi/' -q\n",
    "print('Video downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "videopath='/home/adewopva/Downloads/TestVideosForPhase1(Action Recognition and Prediction)/multiple_activities.mov'\n",
    "modelweight='Kinetics'\n",
    "Allscripts_Recognition(videopath,modelweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip=video_test('/home/adewopva/OneDrive/Independent_Study/Dr.Nelly/ROUGH Scripts/AR_UCF/Testing_Models/Video by ig/splitted-6-16.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_gif(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-reserve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embed('/home/adewopva/Downloads/TestVideosForPhase1(Action Recognition and Prediction)/multiple_activity_dif_babies.mov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-script",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['on_feet', 'active', 'rest', 'escape', 'crawling']\n",
    "clip=video_test('/home/adewopva/OneDrive/Independent_Study/Dr.Nelly/ROUGH Scripts/AR_UCF/Testing_Models/multiple_activities/splitted-0-15.mp4') #FUNCTION TO crop and preprocess video\n",
    "output1= kin_RGB_k.predict(clip)\n",
    "print(output1)\n",
    "\n",
    "kin_RGB_k_Iresult=output1[0]\n",
    "print(kin_RGB_k_Iresult)\n",
    "probabilities1 = tf.nn.softmax(kin_RGB_k_Iresult)\n",
    "#print(kin_RGB_k_Iresultresult)\n",
    "print(\"The Kinetics and Imagenet model topmost three predictions are:\")\n",
    "for i in np.argsort(probabilities1)[::-1][:3]:\n",
    "    print(f\"  {labels[i]:}: {probabilities1[i] * 100:5.2f}%\")\n",
    "\n",
    "print('\\n')\n",
    "to_gif(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-marijuana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def comparemodels(clip):\n",
    "    labels=['on_feet', 'active', 'rest', 'escape', 'crawling']\n",
    "    clip=video_test(clip)\n",
    "    output1= kin_RGB_k_I.predict(clip)\n",
    "    kin_RGB_k_Iresult=output1[0]\n",
    "    probabilities1 = tf.nn.softmax(kin_RGB_k_Iresult)\n",
    "    #print(kin_RGB_k_Iresultresult)\n",
    "    print(\"The Kinetics and Imagenet model topmost three predictions are:\")\n",
    "    for i in np.argsort(probabilities1)[::-1][:3]:\n",
    "        print(f\"  {labels[i]:}: {probabilities1[i] * 100:5.2f}%\")\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    output2= kin_RGB_k.predict(clip)\n",
    "    kin_RGB_kresult=output2[0]\n",
    "    probabilities2 = tf.nn.softmax(kin_RGB_kresult)\n",
    "    #print(kin_RGB_kresultresult)\n",
    "    print(\"The Kinetics only model topmost three predictions are:\")\n",
    "    for i in np.argsort(probabilities2)[::-1][:3]:\n",
    "        print(f\"  {labels[i]:}: {probabilities2[i] * 100:5.2f}%\")\n",
    "        print('\\n')\n",
    "\n",
    "    return to_gif(clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amir's test video\n",
    "comparemodels('/home/adewopva/OneDrive/Independent_Study/Dr.Nelly/ROUGH Scripts/AR_UCF/Testing_Models/Video by sweety_babiez-CNOGgFnA_N_.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amir's test video\n",
    "comparemodels('/home/adewopva/Downloads/TestVideosForPhase1(Action Recognition and Prediction)/multiple_activities.mov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amir's test video\n",
    "comparemodels('/home/adewopva/Downloads/TestVideosForPhase1(Action Recognition and Prediction)/All_in_crib.mov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "Download new videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "!youtube-dl 'https://www.instagram.com/p/CAjFYbWnaxE/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instangram Downloader\n",
    "!instalooter post 'https://www.instagram.com/p/CGgKBEph3ju/'  ~/Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d67a4-229b-4ecb-b94b-8ec350ec75a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-hungary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#play the downloaded video\n",
    "from IPython.display import Video\n",
    "\n",
    "Video('production ID -3676977-388992755.mp4', embed=True, height=240,width=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download using youtube dl\n",
    "!youtube-dl 'https://www.youtube.com/watch?t=4&v=BaW_jenozKc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Links to videos and repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "2000 babies videos\n",
    "https://www.pexels.com/search/videos/baby/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "Instagram baby videos\n",
    "https://www.instagram.com/baby_videos__/?hl=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "YoutubeDl repo\n",
    "https://github.com/ytdl-org/youtube-dl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
